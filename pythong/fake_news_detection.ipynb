{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [],
   "authorship_tag": "ABX9TyPEmBbcU5DFcDF753QI21ez",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import Required Libraries\n",
    "\n",
    "Importing the necessary libraries:\n"
   ],
   "metadata": {
    "id": "fxiA1oNssM2-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers"
   ],
   "metadata": {
    "id": "hs1IMiUVsZzO",
    "ExecuteTime": {
     "end_time": "2024-11-30T19:41:47.895870Z",
     "start_time": "2024-11-30T19:41:45.789929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\ziemm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.40.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ziemm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\ziemm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ziemm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ziemm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ziemm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ziemm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2024.4.16)\n",
      "Requirement already satisfied: requests in c:\\users\\ziemm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\ziemm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\ziemm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ziemm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ziemm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ziemm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ziemm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ziemm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ziemm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ziemm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ziemm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2023.7.22)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "from transformers import DistilBertForSequenceClassification"
   ],
   "metadata": {
    "id": "D9GJ-oofr8ZO",
    "ExecuteTime": {
     "end_time": "2024-11-30T19:46:16.522090Z",
     "start_time": "2024-11-30T19:46:16.506456Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the Dataset\n",
    "\n",
    "Load the dataset into a pandas DataFrame\n"
   ],
   "metadata": {
    "id": "GcBVA15rscR3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load train data\n",
    "train_data = pd.read_csv('https://raw.githubusercontent.com/salarMokhtariL/Facke-News-Detection/main/Dataset/train.csv')\n",
    "\n",
    "# Load test data\n",
    "test_data = pd.read_csv('https://raw.githubusercontent.com/salarMokhtariL/Facke-News-Detection/main/Dataset/test.csv')"
   ],
   "metadata": {
    "id": "zxuSojQnsYLG",
    "ExecuteTime": {
     "end_time": "2024-11-30T19:46:45.104652Z",
     "start_time": "2024-11-30T19:46:27.369710Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "train_data.dropna(inplace=True)\n"
   ],
   "metadata": {
    "id": "c0-eeQKO6HIC",
    "ExecuteTime": {
     "end_time": "2024-11-30T19:47:09.411663Z",
     "start_time": "2024-11-30T19:47:09.391180Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare the Data\n",
    "Prepare the data for the PyTorch model. First, let's define a custom dataset class"
   ],
   "metadata": {
    "id": "t9h859kos6k1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "''' This class takes in the data, tokenizes it using the DistilBertTokenizer from the transformers library,\n",
    " and returns the input IDs, attention masks, and labels.'''\n",
    "\n",
    "\n",
    "class FakeNewsDataset(Dataset):\n",
    "    def __init__(self, data, max_len=128):\n",
    "        self.data = data\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.data.iloc[index]['text']\n",
    "        label = self.data.iloc[index]['label']\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return inputs['input_ids'].squeeze(0), inputs['attention_mask'].squeeze(0), torch.tensor(label, dtype=torch.long)"
   ],
   "metadata": {
    "id": "9-4ULm_Ps1f_",
    "ExecuteTime": {
     "end_time": "2024-11-30T19:47:12.300608Z",
     "start_time": "2024-11-30T19:47:12.290241Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "# split the data into training and validation sets\n",
    "\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2,\n",
    "                                        random_state=42)"
   ],
   "metadata": {
    "id": "D3t0jbkdtV02",
    "ExecuteTime": {
     "end_time": "2024-11-30T19:47:14.213618Z",
     "start_time": "2024-11-30T19:47:14.182360Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "# Create PyTorch data loaders for the training, validation, and test sets:\n",
    "\n",
    "train_dataset = FakeNewsDataset(train_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = FakeNewsDataset(val_data)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_dataset = FakeNewsDataset(test_data)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ],
   "metadata": {
    "id": "qoRnpQ_4tYr6",
    "ExecuteTime": {
     "end_time": "2024-11-30T19:47:17.836931Z",
     "start_time": "2024-11-30T19:47:17.162979Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define the Model\n",
    "define the PyTorch model. We'll use the `DistilBertForSequenceClassification` model from the `transformers` library:"
   ],
   "metadata": {
    "id": "uXoyz5NiuR7x"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class FakeNewsClassifier(nn.Module):\n",
    "    def __init__(self, num_labels=2):\n",
    "        super(FakeNewsClassifier, self).__init__()\n",
    "        self.bert = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs[0]"
   ],
   "metadata": {
    "id": "e5T1YcxauKpe",
    "ExecuteTime": {
     "end_time": "2024-11-30T19:48:50.724306Z",
     "start_time": "2024-11-30T19:48:50.705719Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train the Model\n",
    "\n",
    "With the data and model prepared, we can now train the model using PyTorch. We'll define a function to train the model for one epoch"
   ],
   "metadata": {
    "id": "T5opyKlLujgq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def train_epoch(model, optimizer, criterion, train_loader):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "\n",
    "    for input_ids, attention_mask, labels in tqdm(train_loader, desc='Training'):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device))\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += (outputs.argmax(1) == labels.to(device)).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc /= len(train_loader.dataset)\n",
    "\n",
    "    return train_loss, train_acc"
   ],
   "metadata": {
    "id": "MJT_-swAujJn",
    "ExecuteTime": {
     "end_time": "2024-11-30T19:48:55.205939Z",
     "start_time": "2024-11-30T19:48:55.190350Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "This function takes in the model, optimizer, loss function, and data loader, and performs a forward pass through the model, calculates the loss, and performs backpropagation and gradient descent to update the model parameters"
   ],
   "metadata": {
    "id": "YRTL30JYusZy"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll also define a function to evaluate the model on the validation set:"
   ],
   "metadata": {
    "id": "uXAZkgv8utqw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def eval_epoch(model, criterion, val_loader):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels in tqdm(val_loader, desc='Validation'):\n",
    "            outputs = model(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device))\n",
    "            loss = criterion(outputs, labels.to(device))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_acc += (outputs.argmax(1) == labels.to(device)).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc /= len(val_loader.dataset)\n",
    "\n",
    "    return val_loss, val_acc"
   ],
   "metadata": {
    "id": "Fz5hGObLucd8",
    "ExecuteTime": {
     "end_time": "2024-11-30T19:49:01.359528Z",
     "start_time": "2024-11-30T19:49:01.343919Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "This function takes in the model, loss function, and data loader, and performs a forward pass through the model to calculate the loss and accuracy on the validation set."
   ],
   "metadata": {
    "id": "tfYQ5ey_uwhZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can define the main training loop:"
   ],
   "metadata": {
    "id": "Gaq98MZWuyDE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = FakeNewsClassifier().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_val_acc = 0\n",
    "\n",
    "for epoch in range(5):\n",
    "    train_loss, train_acc = train_epoch(model, optimizer, criterion, train_loader)\n",
    "    val_loss, val_acc = eval_epoch(model, criterion, val_loader)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}')\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "        best_val_acc = val_acc"
   ],
   "metadata": {
    "id": "ZQCu1YIQuvTm",
    "ExecuteTime": {
     "end_time": "2024-11-30T19:55:08.299030Z",
     "start_time": "2024-11-30T19:49:03.679046Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training:  11%|█         | 49/458 [06:02<50:26,  7.40s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 10\u001B[0m\n\u001B[0;32m      7\u001B[0m best_val_acc \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m5\u001B[39m):\n\u001B[1;32m---> 10\u001B[0m     train_loss, train_acc \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m     val_loss, val_acc \u001B[38;5;241m=\u001B[39m eval_epoch(model, criterion, val_loader)\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: Train Loss=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Train Acc=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_acc\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Val Loss=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Val Acc=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval_acc\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[1;32mIn[10], line 10\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[1;34m(model, optimizer, criterion, train_loader)\u001B[0m\n\u001B[0;32m      8\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(input_ids\u001B[38;5;241m=\u001B[39minput_ids\u001B[38;5;241m.\u001B[39mto(device), attention_mask\u001B[38;5;241m=\u001B[39mattention_mask\u001B[38;5;241m.\u001B[39mto(device))\n\u001B[0;32m      9\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels\u001B[38;5;241m.\u001B[39mto(device))\n\u001B[1;32m---> 10\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     13\u001B[0m train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py:522\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    512\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    513\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    514\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    515\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    520\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    521\u001B[0m     )\n\u001B[1;32m--> 522\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    524\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    261\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    263\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    264\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    265\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 266\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    274\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "# Train and evaluate the model\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Train\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        batch_inputs, batch_masks, batch_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_inputs, attention_mask=batch_masks)\n",
    "        loss = loss_fn(outputs, batch_labels)\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1} / {epochs} - Batch {step} / {len(train_loader)} - Loss: {loss.item()}\")\n",
    "\n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    total_validation_loss = 0\n",
    "    predictions, true_labels = [], []\n",
    "    for batch in val_loader:\n",
    "        batch_inputs, batch_masks, batch_labels = batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch_inputs, attention_mask=batch_masks)\n",
    "            loss = loss_fn(outputs, batch_labels)\n",
    "            total_validation_loss += loss.item()\n",
    "            predictions += list(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "            true_labels += list(batch_labels.cpu().numpy())\n",
    "\n",
    "    # Print training and validation loss\n",
    "    average_train_loss = total_train_loss / len(train_loader)\n",
    "    average_validation_loss = total_validation_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1} / {epochs} - Average training loss: {average_train_loss}\")\n",
    "    print(f\"Epoch {epoch+1} / {epochs} - Average validation loss: {average_validation_loss}\")\n",
    "\n",
    "    # Print classification report\n",
    "    print(classification_report(true_labels, predictions))\n",
    "\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), f\"distilbert-fake-news-{epoch+1}.pth\")"
   ],
   "metadata": {
    "id": "l49LSMPT2_Qq"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
